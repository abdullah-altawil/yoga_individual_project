{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02e63e1f-6c80-4659-9481-352c76a5e9c6",
      "metadata": {
        "id": "02e63e1f-6c80-4659-9481-352c76a5e9c6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "import optuna\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687eb756-bbbc-46e7-9786-367467f9ba8c",
      "metadata": {
        "id": "687eb756-bbbc-46e7-9786-367467f9ba8c"
      },
      "outputs": [],
      "source": [
        "file_path = '../data/combined_keypoints.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Class'] = le.fit_transform(df['Class'])\n",
        "\n",
        "X = df.drop('Class', axis=1).values\n",
        "y = df['Class'].values\n",
        "\n",
        "def convert_to_geometric(data, labels):\n",
        "    data_list = []\n",
        "    for i in range(len(data)):\n",
        "        x = torch.tensor(data[i].reshape(-1, 3), dtype=torch.float)\n",
        "        edge_index = torch.tensor([[i, i+1] for i in range(len(x)-1)], dtype=torch.long).t().contiguous()\n",
        "        y = torch.tensor(labels[i], dtype=torch.long)\n",
        "        data_list.append(Data(x=x, edge_index=edge_index, y=y))\n",
        "    return data_list\n",
        "\n",
        "data_list = convert_to_geometric(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e549065-d919-4efa-9567-b86a482498cf",
      "metadata": {
        "id": "2e549065-d919-4efa-9567-b86a482498cf",
        "outputId": "4a585180-558e-4bf7-fbc1-79cb72344d0c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/abdulah/anaconda3/lib/python3.11/site-packages/torch_geometric/deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
            "  warnings.warn(out)\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42)\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c39d619-b522-49c0-b4d2-ea8f129bdb20",
      "metadata": {
        "id": "7c39d619-b522-49c0-b4d2-ea8f129bdb20"
      },
      "outputs": [],
      "source": [
        "class YogaPoseGNN(torch.nn.Module):\n",
        "    def __init__(self, num_classes, hidden_dim=64, num_layers=3, dropout_rate=0.5):\n",
        "        super(YogaPoseGNN, self).__init__()\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(3, hidden_dim))\n",
        "        for _ in range(num_layers - 1):\n",
        "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
        "        self.fc1 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, num_classes)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout_rate)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "        for conv in self.convs:\n",
        "            x = conv(x, edge_index)\n",
        "            x = F.relu(x)\n",
        "        x = global_mean_pool(x, batch)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "def train(model, optimizer, criterion, train_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            out = model(data)\n",
        "            pred = out.argmax(dim=1)\n",
        "            correct += (pred == data.y).sum().item()\n",
        "            all_preds.extend(pred.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "    return correct / len(loader.dataset), all_preds, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ed93de8-9021-486c-8f69-fd9b7b87b627",
      "metadata": {
        "id": "1ed93de8-9021-486c-8f69-fd9b7b87b627",
        "outputId": "07240c63-f431-42e8-aeb4-aca0f27c86a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2024-08-04 16:59:08,881] A new study created in memory with name: no-name-8780294e-88ba-4517-9f0a-ea218244c8e4\n",
            "/tmp/ipykernel_20421/515450261.py:3: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
            "/tmp/ipykernel_20421/515450261.py:4: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
            "/tmp/ipykernel_20421/515450261.py:5: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.7)\n",
            "/tmp/ipykernel_20421/515450261.py:11: FutureWarning: suggest_uniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float instead.\n",
            "  gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
            "[I 2024-08-04 17:06:41,097] Trial 0 finished with value: 0.9490167516387473 and parameters: {'lr': 0.0017054177100578527, 'weight_decay': 0.0018769003460625251, 'dropout_rate': 0.5865356673670512, 'num_layers': 4, 'hidden_dim': 125, 'batch_size': 32, 'optimizer': 'SGD', 'step_size': 44, 'gamma': 0.22315324232280906}. Best is trial 0 with value: 0.9490167516387473.\n",
            "[I 2024-08-04 17:16:45,157] Trial 1 finished with value: 0.7858703568827385 and parameters: {'lr': 0.00021064405875542935, 'weight_decay': 1.0580312071561183e-05, 'dropout_rate': 0.48337552979509846, 'num_layers': 2, 'hidden_dim': 85, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 27, 'gamma': 0.7053514952148989}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 17:29:06,228] Trial 2 finished with value: 0.8142753095411508 and parameters: {'lr': 0.0006694702610712913, 'weight_decay': 0.00014956086901033138, 'dropout_rate': 0.4699065106839854, 'num_layers': 3, 'hidden_dim': 36, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 16, 'gamma': 0.4505902212206062}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 17:38:12,247] Trial 3 finished with value: 0.8033503277494538 and parameters: {'lr': 0.0002824148363576546, 'weight_decay': 0.00019478630846766175, 'dropout_rate': 0.4432983054826366, 'num_layers': 4, 'hidden_dim': 39, 'batch_size': 32, 'optimizer': 'Adam', 'step_size': 43, 'gamma': 0.6396713598282824}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 17:55:20,879] Trial 4 finished with value: 0.8270211216314639 and parameters: {'lr': 0.0003636872575673978, 'weight_decay': 0.00253479606517482, 'dropout_rate': 0.4997988380199423, 'num_layers': 4, 'hidden_dim': 73, 'batch_size': 32, 'optimizer': 'Adam', 'step_size': 43, 'gamma': 0.1721203541713276}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:12:51,676] Trial 5 finished with value: 0.7895120174799709 and parameters: {'lr': 0.0008395111075969503, 'weight_decay': 0.0008162581319774843, 'dropout_rate': 0.687582460809351, 'num_layers': 4, 'hidden_dim': 63, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 44, 'gamma': 0.5547548895305865}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:31:12,354] Trial 6 finished with value: 0.9646758922068464 and parameters: {'lr': 0.0016180014416070484, 'weight_decay': 0.0091821921973507, 'dropout_rate': 0.5341164624253433, 'num_layers': 5, 'hidden_dim': 60, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 16, 'gamma': 0.10290847279850707}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:36:12,671] Trial 7 finished with value: 0.9286234522942461 and parameters: {'lr': 0.0026223211432940163, 'weight_decay': 3.1590420699251844e-05, 'dropout_rate': 0.5638699821824549, 'num_layers': 2, 'hidden_dim': 115, 'batch_size': 32, 'optimizer': 'SGD', 'step_size': 31, 'gamma': 0.3304070041097261}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:47:01,787] Trial 8 finished with value: 0.8102694828841952 and parameters: {'lr': 0.0003201378446735158, 'weight_decay': 0.0006021951745900714, 'dropout_rate': 0.6235116185896017, 'num_layers': 5, 'hidden_dim': 40, 'batch_size': 32, 'optimizer': 'Adam', 'step_size': 47, 'gamma': 0.8425221913228967}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:56:09,103] Trial 9 finished with value: 0.9646758922068464 and parameters: {'lr': 0.00012333710019978234, 'weight_decay': 0.0003328571772418308, 'dropout_rate': 0.5269786353233448, 'num_layers': 3, 'hidden_dim': 69, 'batch_size': 16, 'optimizer': 'SGD', 'step_size': 13, 'gamma': 0.7927537031364176}. Best is trial 1 with value: 0.7858703568827385.\n",
            "[I 2024-08-04 18:59:32,636] Trial 10 finished with value: 0.7552804078659869 and parameters: {'lr': 0.00918456619159633, 'weight_decay': 1.0422288758321607e-05, 'dropout_rate': 0.2960787931234619, 'num_layers': 2, 'hidden_dim': 97, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 28, 'gamma': 0.6748985030681438}. Best is trial 10 with value: 0.7552804078659869.\n",
            "[I 2024-08-04 19:02:54,871] Trial 11 finished with value: 0.7563729060451566 and parameters: {'lr': 0.00968358094484462, 'weight_decay': 1.0253892912343e-05, 'dropout_rate': 0.304834022603897, 'num_layers': 2, 'hidden_dim': 97, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 27, 'gamma': 0.6798128916224315}. Best is trial 10 with value: 0.7552804078659869.\n",
            "[I 2024-08-04 19:06:13,222] Trial 12 finished with value: 0.746176256372906 and parameters: {'lr': 0.009299603373250096, 'weight_decay': 1.0446330395785242e-05, 'dropout_rate': 0.2829860837758841, 'num_layers': 2, 'hidden_dim': 96, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 28, 'gamma': 0.6773823056144644}. Best is trial 12 with value: 0.746176256372906.\n",
            "[I 2024-08-04 19:10:29,926] Trial 13 finished with value: 0.6809905316824472 and parameters: {'lr': 0.00937168580358029, 'weight_decay': 5.076342347712558e-05, 'dropout_rate': 0.20579142896921038, 'num_layers': 3, 'hidden_dim': 101, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 34, 'gamma': 0.5268161717831789}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:14:50,811] Trial 14 finished with value: 0.822286962855062 and parameters: {'lr': 0.0049828653852611385, 'weight_decay': 5.411311062086836e-05, 'dropout_rate': 0.21394076004110102, 'num_layers': 3, 'hidden_dim': 105, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 35, 'gamma': 0.4543907005117107}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:18:52,266] Trial 15 finished with value: 0.84122359796067 and parameters: {'lr': 0.004603433887279265, 'weight_decay': 4.488950526732445e-05, 'dropout_rate': 0.20211915082558443, 'num_layers': 3, 'hidden_dim': 87, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 35, 'gamma': 0.5562009939107041}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:22:22,927] Trial 16 finished with value: 0.9431900946831755 and parameters: {'lr': 0.004793059931574517, 'weight_decay': 3.183330876635692e-05, 'dropout_rate': 0.36519582823325153, 'num_layers': 2, 'hidden_dim': 109, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 22, 'gamma': 0.40611341732565737}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:26:57,125] Trial 17 finished with value: 0.8259286234522942 and parameters: {'lr': 0.0031347121881791207, 'weight_decay': 8.725598803000097e-05, 'dropout_rate': 0.2730116346532626, 'num_layers': 3, 'hidden_dim': 121, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 35, 'gamma': 0.8992176612321352}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:30:20,122] Trial 18 finished with value: 0.8164603058994901 and parameters: {'lr': 0.009940050255031157, 'weight_decay': 2.1956831610403852e-05, 'dropout_rate': 0.3914674000607473, 'num_layers': 2, 'hidden_dim': 95, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 23, 'gamma': 0.5706009968508922}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:33:51,988] Trial 19 finished with value: 0.7363437727603788 and parameters: {'lr': 0.006195534750245896, 'weight_decay': 9.436779076060267e-05, 'dropout_rate': 0.2553838851196782, 'num_layers': 3, 'hidden_dim': 52, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 39, 'gamma': 0.7627339949110513}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:37:24,237] Trial 20 finished with value: 0.9519300801165331 and parameters: {'lr': 0.0014430570495366773, 'weight_decay': 0.00013224450704530044, 'dropout_rate': 0.3622902124912167, 'num_layers': 3, 'hidden_dim': 52, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 38, 'gamma': 0.7665558311990741}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:41:18,044] Trial 21 finished with value: 0.7046613255644574 and parameters: {'lr': 0.006509084382836127, 'weight_decay': 6.320880750544074e-05, 'dropout_rate': 0.25303875412069565, 'num_layers': 3, 'hidden_dim': 78, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 40, 'gamma': 0.7620954725083724}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:44:49,743] Trial 22 finished with value: 0.7418062636562273 and parameters: {'lr': 0.006008986891189916, 'weight_decay': 7.770756766196365e-05, 'dropout_rate': 0.23452962124243698, 'num_layers': 3, 'hidden_dim': 52, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 50, 'gamma': 0.7610287389825651}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:48:59,991] Trial 23 finished with value: 0.8579752367079388 and parameters: {'lr': 0.00297673536065566, 'weight_decay': 0.00029857675119235663, 'dropout_rate': 0.2481052535270638, 'num_layers': 3, 'hidden_dim': 76, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 40, 'gamma': 0.8678396587595171}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:56:15,228] Trial 24 finished with value: 0.7563729060451566 and parameters: {'lr': 0.00621773377161366, 'weight_decay': 8.292487279772916e-05, 'dropout_rate': 0.32643693436216015, 'num_layers': 4, 'hidden_dim': 83, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 39, 'gamma': 0.3469890577778544}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 19:59:47,402] Trial 25 finished with value: 0.8634377276037873 and parameters: {'lr': 0.0035953895749120234, 'weight_decay': 2.1490680419690334e-05, 'dropout_rate': 0.2506500194066225, 'num_layers': 3, 'hidden_dim': 51, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 31, 'gamma': 0.6265387729733782}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 20:04:04,706] Trial 26 finished with value: 0.85360524399126 and parameters: {'lr': 0.0022004523998496507, 'weight_decay': 0.00024969895080934865, 'dropout_rate': 0.4146103599434655, 'num_layers': 4, 'hidden_dim': 66, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 37, 'gamma': 0.8237913860049497}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 20:08:21,204] Trial 27 finished with value: 0.7563729060451566 and parameters: {'lr': 0.007184767591559343, 'weight_decay': 0.00010478481751629059, 'dropout_rate': 0.3295467977412479, 'num_layers': 3, 'hidden_dim': 105, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 32, 'gamma': 0.5081389327106064}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 20:12:20,689] Trial 28 finished with value: 0.8095411507647488 and parameters: {'lr': 0.00427764211082392, 'weight_decay': 5.1096192872916037e-05, 'dropout_rate': 0.23484141514314863, 'num_layers': 3, 'hidden_dim': 89, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 41, 'gamma': 0.7561231264756155}. Best is trial 13 with value: 0.6809905316824472.\n",
            "[I 2024-08-04 20:17:54,880] Trial 29 finished with value: 0.6325564457392572 and parameters: {'lr': 0.006899563880774035, 'weight_decay': 2.610264590093146e-05, 'dropout_rate': 0.20591923852703245, 'num_layers': 4, 'hidden_dim': 128, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 46, 'gamma': 0.2661927678915462}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 20:26:40,291] Trial 30 finished with value: 0.8787327021121631 and parameters: {'lr': 0.0010590485359703637, 'weight_decay': 1.8216961170834218e-05, 'dropout_rate': 0.2024351913195494, 'num_layers': 5, 'hidden_dim': 127, 'batch_size': 32, 'optimizer': 'SGD', 'step_size': 47, 'gamma': 0.29469327520879346}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 20:32:12,398] Trial 31 finished with value: 0.6955571740713766 and parameters: {'lr': 0.006772023943010405, 'weight_decay': 3.356646816614586e-05, 'dropout_rate': 0.26512242684875303, 'num_layers': 4, 'hidden_dim': 119, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 47, 'gamma': 0.19806284391440715}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 20:37:36,918] Trial 32 finished with value: 0.6846321922796795 and parameters: {'lr': 0.007213174055944858, 'weight_decay': 3.394972458058927e-05, 'dropout_rate': 0.3260926446593292, 'num_layers': 4, 'hidden_dim': 120, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 47, 'gamma': 0.20580853428869642}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 20:42:59,664] Trial 33 finished with value: 0.6635105608157319 and parameters: {'lr': 0.007229199861253042, 'weight_decay': 3.234310390015891e-05, 'dropout_rate': 0.32650852615166515, 'num_layers': 4, 'hidden_dim': 120, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 50, 'gamma': 0.23096594824318536}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 20:54:38,516] Trial 34 finished with value: 0.6416605972323379 and parameters: {'lr': 0.002015494604442814, 'weight_decay': 1.7229968252390057e-05, 'dropout_rate': 0.3403397900588103, 'num_layers': 4, 'hidden_dim': 128, 'batch_size': 16, 'optimizer': 'SGD', 'step_size': 50, 'gamma': 0.24064913449153824}. Best is trial 29 with value: 0.6325564457392572.\n",
            "[I 2024-08-04 21:29:23,332] Trial 35 finished with value: 0.5695557174071377 and parameters: {'lr': 0.0005253349961281477, 'weight_decay': 1.776618474427631e-05, 'dropout_rate': 0.3636132094412057, 'num_layers': 5, 'hidden_dim': 127, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 50, 'gamma': 0.2692244103661817}. Best is trial 35 with value: 0.5695557174071377.\n",
            "[W 2024-08-04 21:51:51,678] Trial 36 failed with parameters: {'lr': 0.0006574045422800642, 'weight_decay': 1.6289680136266676e-05, 'dropout_rate': 0.4180898754849577, 'num_layers': 5, 'hidden_dim': 127, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 50, 'gamma': 0.26241597805969896} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 196, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_20421/515450261.py\", line 29, in objective\n",
            "    train_acc, _, _ = test(model, train_loader)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_20421/149891805.py\", line 45, in test\n",
            "    out = model(data)\n",
            "          ^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipykernel_20421/149891805.py\", line 15, in forward\n",
            "    x = conv(x, edge_index)\n",
            "        ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py\", line 260, in forward\n",
            "    x = self.lin(x)\n",
            "        ^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/abdulah/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py\", line 147, in forward\n",
            "    return F.linear(x, self.weight, self.bias)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2024-08-04 21:51:51,719] Trial 36 failed with value None.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Create and run the Optuna study\u001b[39;00m\n\u001b[1;32m     36\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)  \u001b[38;5;66;03m# Increase the number of trials\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m1\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstudy\u001b[38;5;241m.\u001b[39mbest_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     _optimize(\n\u001b[1;32m    452\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    453\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    454\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    455\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    456\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    457\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    458\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    459\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    460\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    461\u001b[0m     )\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m         _optimize_sequential(\n\u001b[1;32m     63\u001b[0m             study,\n\u001b[1;32m     64\u001b[0m             func,\n\u001b[1;32m     65\u001b[0m             n_trials,\n\u001b[1;32m     66\u001b[0m             timeout,\n\u001b[1;32m     67\u001b[0m             catch,\n\u001b[1;32m     68\u001b[0m             callbacks,\n\u001b[1;32m     69\u001b[0m             gc_after_trial,\n\u001b[1;32m     70\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     71\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    246\u001b[0m ):\n\u001b[0;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
            "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m101\u001b[39m):\n\u001b[1;32m     28\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train(model, optimizer, criterion, train_loader)\n\u001b[0;32m---> 29\u001b[0m     train_acc, _, _ \u001b[38;5;241m=\u001b[39m test(model, train_loader)\n\u001b[1;32m     30\u001b[0m     test_acc, _, _ \u001b[38;5;241m=\u001b[39m test(model, test_loader)\n\u001b[1;32m     31\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
            "Cell \u001b[0;32mIn[4], line 45\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m---> 45\u001b[0m         out \u001b[38;5;241m=\u001b[39m model(data)\n\u001b[1;32m     46\u001b[0m         pred \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     47\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (pred \u001b[38;5;241m==\u001b[39m data\u001b[38;5;241m.\u001b[39my)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[4], line 15\u001b[0m, in \u001b[0;36mYogaPoseGNN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     13\u001b[0m x, edge_index, batch \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index, data\u001b[38;5;241m.\u001b[39mbatch\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m conv \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvs:\n\u001b[0;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m conv(x, edge_index)\n\u001b[1;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m global_mean_pool(x, batch)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:260\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m             edge_index \u001b[38;5;241m=\u001b[39m cache\n\u001b[0;32m--> 260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin(x)\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[1;32m    263\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpropagate(edge_index, x\u001b[38;5;241m=\u001b[39mx, edge_weight\u001b[38;5;241m=\u001b[39medge_weight)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch_geometric/nn/dense/linear.py:147\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    142\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;03m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def objective(trial):\n",
        "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-2)\n",
        "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-5, 1e-2)\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.2, 0.7)\n",
        "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
        "    hidden_dim = trial.suggest_int('hidden_dim', 32, 128)\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
        "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'SGD'])\n",
        "    step_size = trial.suggest_int('step_size', 10, 50)\n",
        "    gamma = trial.suggest_uniform('gamma', 0.1, 0.9)\n",
        "\n",
        "    model = YogaPoseGNN(num_classes=len(le.classes_), hidden_dim=hidden_dim, num_layers=num_layers, dropout_rate=dropout_rate)\n",
        "\n",
        "    if optimizer_name == 'Adam':\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    else:\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=weight_decay, momentum=0.9)\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "      train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    for epoch in range(1, 101):\n",
        "        train_loss = train(model, optimizer, criterion, train_loader)\n",
        "        train_acc, _, _ = test(model, train_loader)\n",
        "        test_acc, _, _ = test(model, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    return 1 - test_acc\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print(f'Best parameters: {study.best_params}')\n",
        "print(f'Best accuracy: {1 - study.best_value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e7d050-9eb8-43b8-bf9a-53d41f01b3a8",
      "metadata": {
        "id": "b9e7d050-9eb8-43b8-bf9a-53d41f01b3a8"
      },
      "outputs": [],
      "source": [
        "Trial 29 finished with value: 0.6325564457392572 and parameters: {'lr': 0.006899563880774035, 'weight_decay': 2.610264590093146e-05, 'dropout_rate': 0.20591923852703245, 'num_layers': 4, 'hidden_dim': 128, 'batch_size': 64, 'optimizer': 'SGD', 'step_size': 46, 'gamma': 0.2661927678915462}. Best is trial 29 with value: 0.6325564457392572."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c854b080-b9eb-4699-8f0d-731ea5720f86",
      "metadata": {
        "id": "c854b080-b9eb-4699-8f0d-731ea5720f86",
        "outputId": "2c987cda-bf70-4874-b95a-14f4c279da97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'lr': 0.0005253349961281477, 'weight_decay': 1.776618474427631e-05, 'dropout_rate': 0.3636132094412057, 'num_layers': 5, 'hidden_dim': 127, 'batch_size': 16, 'optimizer': 'Adam', 'step_size': 50, 'gamma': 0.2692244103661817}\n"
          ]
        }
      ],
      "source": [
        "print(f'Best parameters: {study.best_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a40d8612-adb1-420a-9ee5-8fb0b524989a",
      "metadata": {
        "id": "a40d8612-adb1-420a-9ee5-8fb0b524989a",
        "outputId": "d07fb78d-d8a6-4c61-893d-adaa3d5b1847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2: Loss: 4.2192, Train Acc: 0.0536, Test Acc: 0.0528\n",
            "Epoch 3: Loss: 3.9639, Train Acc: 0.0870, Test Acc: 0.0914\n",
            "Epoch 4: Loss: 3.6746, Train Acc: 0.1182, Test Acc: 0.1245\n",
            "Epoch 5: Loss: 3.5122, Train Acc: 0.1279, Test Acc: 0.1351\n",
            "Epoch 6: Loss: 3.4285, Train Acc: 0.1487, Test Acc: 0.1395\n",
            "Epoch 7: Loss: 3.3680, Train Acc: 0.1417, Test Acc: 0.1482\n",
            "Epoch 8: Loss: 3.3235, Train Acc: 0.1572, Test Acc: 0.1577\n",
            "Epoch 9: Loss: 3.2796, Train Acc: 0.1595, Test Acc: 0.1642\n",
            "Epoch 10: Loss: 3.2394, Train Acc: 0.1769, Test Acc: 0.1737\n",
            "Epoch 11: Loss: 3.1982, Train Acc: 0.1964, Test Acc: 0.1948\n",
            "Epoch 12: Loss: 3.1606, Train Acc: 0.2067, Test Acc: 0.1923\n",
            "Epoch 13: Loss: 3.1285, Train Acc: 0.2084, Test Acc: 0.2014\n",
            "Epoch 14: Loss: 3.0969, Train Acc: 0.2174, Test Acc: 0.2156\n",
            "Epoch 15: Loss: 3.0687, Train Acc: 0.2278, Test Acc: 0.2203\n",
            "Epoch 16: Loss: 3.0443, Train Acc: 0.2281, Test Acc: 0.2072\n",
            "Epoch 17: Loss: 3.0018, Train Acc: 0.2183, Test Acc: 0.2145\n",
            "Epoch 18: Loss: 2.9864, Train Acc: 0.2217, Test Acc: 0.2061\n",
            "Epoch 19: Loss: 2.9550, Train Acc: 0.2392, Test Acc: 0.2258\n",
            "Epoch 20: Loss: 2.9402, Train Acc: 0.2486, Test Acc: 0.2385\n",
            "Epoch 21: Loss: 2.9083, Train Acc: 0.2633, Test Acc: 0.2491\n",
            "Epoch 22: Loss: 2.8853, Train Acc: 0.2635, Test Acc: 0.2513\n",
            "Epoch 23: Loss: 2.8576, Train Acc: 0.2800, Test Acc: 0.2538\n",
            "Epoch 24: Loss: 2.8338, Train Acc: 0.2787, Test Acc: 0.2662\n",
            "Epoch 25: Loss: 2.8064, Train Acc: 0.2898, Test Acc: 0.2666\n",
            "Epoch 26: Loss: 2.7827, Train Acc: 0.2941, Test Acc: 0.2706\n",
            "Epoch 27: Loss: 2.7602, Train Acc: 0.3066, Test Acc: 0.2811\n",
            "Epoch 28: Loss: 2.7283, Train Acc: 0.3047, Test Acc: 0.2760\n",
            "Epoch 29: Loss: 2.7241, Train Acc: 0.3012, Test Acc: 0.2739\n",
            "Epoch 30: Loss: 2.6903, Train Acc: 0.3268, Test Acc: 0.2968\n",
            "Epoch 31: Loss: 2.6790, Train Acc: 0.3236, Test Acc: 0.2986\n",
            "Epoch 32: Loss: 2.6532, Train Acc: 0.3287, Test Acc: 0.3088\n",
            "Epoch 33: Loss: 2.6222, Train Acc: 0.3386, Test Acc: 0.3157\n",
            "Epoch 34: Loss: 2.6166, Train Acc: 0.3471, Test Acc: 0.3205\n",
            "Epoch 35: Loss: 2.5823, Train Acc: 0.3497, Test Acc: 0.3154\n",
            "Epoch 36: Loss: 2.5821, Train Acc: 0.3384, Test Acc: 0.3084\n",
            "Epoch 37: Loss: 2.5690, Train Acc: 0.3451, Test Acc: 0.3143\n",
            "Epoch 38: Loss: 2.5473, Train Acc: 0.3487, Test Acc: 0.3154\n",
            "Epoch 39: Loss: 2.5156, Train Acc: 0.3605, Test Acc: 0.3274\n",
            "Epoch 40: Loss: 2.5099, Train Acc: 0.3650, Test Acc: 0.3350\n",
            "Epoch 41: Loss: 2.4868, Train Acc: 0.3794, Test Acc: 0.3452\n",
            "Epoch 42: Loss: 2.4842, Train Acc: 0.3932, Test Acc: 0.3609\n",
            "Epoch 43: Loss: 2.4607, Train Acc: 0.3891, Test Acc: 0.3598\n",
            "Epoch 44: Loss: 2.4369, Train Acc: 0.3851, Test Acc: 0.3525\n",
            "Epoch 45: Loss: 2.4262, Train Acc: 0.4008, Test Acc: 0.3572\n",
            "Epoch 46: Loss: 2.4107, Train Acc: 0.3876, Test Acc: 0.3620\n",
            "Epoch 47: Loss: 2.3966, Train Acc: 0.3976, Test Acc: 0.3587\n",
            "Epoch 48: Loss: 2.3754, Train Acc: 0.4063, Test Acc: 0.3602\n",
            "Epoch 49: Loss: 2.3701, Train Acc: 0.4148, Test Acc: 0.3645\n",
            "Epoch 50: Loss: 2.3435, Train Acc: 0.4117, Test Acc: 0.3674\n",
            "Epoch 51: Loss: 2.3510, Train Acc: 0.4095, Test Acc: 0.3740\n",
            "Epoch 52: Loss: 2.2447, Train Acc: 0.4434, Test Acc: 0.3933\n",
            "Epoch 53: Loss: 2.2315, Train Acc: 0.4459, Test Acc: 0.3933\n",
            "Epoch 54: Loss: 2.2116, Train Acc: 0.4466, Test Acc: 0.3958\n",
            "Epoch 55: Loss: 2.2065, Train Acc: 0.4491, Test Acc: 0.3980\n",
            "Epoch 56: Loss: 2.1994, Train Acc: 0.4572, Test Acc: 0.4013\n",
            "Epoch 57: Loss: 2.1992, Train Acc: 0.4572, Test Acc: 0.4046\n",
            "Epoch 58: Loss: 2.1824, Train Acc: 0.4531, Test Acc: 0.4024\n",
            "Epoch 59: Loss: 2.1893, Train Acc: 0.4606, Test Acc: 0.3991\n",
            "Epoch 60: Loss: 2.1820, Train Acc: 0.4625, Test Acc: 0.3973\n",
            "Epoch 61: Loss: 2.1839, Train Acc: 0.4552, Test Acc: 0.3966\n",
            "Epoch 62: Loss: 2.1849, Train Acc: 0.4525, Test Acc: 0.3966\n",
            "Epoch 63: Loss: 2.1722, Train Acc: 0.4652, Test Acc: 0.4009\n",
            "Epoch 64: Loss: 2.1650, Train Acc: 0.4611, Test Acc: 0.3944\n",
            "Epoch 65: Loss: 2.1752, Train Acc: 0.4694, Test Acc: 0.4035\n",
            "Epoch 66: Loss: 2.1554, Train Acc: 0.4670, Test Acc: 0.4013\n",
            "Epoch 67: Loss: 2.1585, Train Acc: 0.4659, Test Acc: 0.4017\n",
            "Epoch 68: Loss: 2.1593, Train Acc: 0.4713, Test Acc: 0.3999\n",
            "Epoch 69: Loss: 2.1524, Train Acc: 0.4660, Test Acc: 0.4039\n",
            "Epoch 70: Loss: 2.1339, Train Acc: 0.4711, Test Acc: 0.4046\n",
            "Epoch 71: Loss: 2.1363, Train Acc: 0.4703, Test Acc: 0.4050\n",
            "Epoch 72: Loss: 2.1352, Train Acc: 0.4787, Test Acc: 0.4075\n",
            "Epoch 73: Loss: 2.1256, Train Acc: 0.4796, Test Acc: 0.4071\n",
            "Epoch 74: Loss: 2.1314, Train Acc: 0.4742, Test Acc: 0.4111\n",
            "Epoch 75: Loss: 2.1270, Train Acc: 0.4816, Test Acc: 0.4097\n",
            "Epoch 76: Loss: 2.1218, Train Acc: 0.4771, Test Acc: 0.4017\n",
            "Epoch 77: Loss: 2.1149, Train Acc: 0.4790, Test Acc: 0.4159\n",
            "Epoch 78: Loss: 2.1167, Train Acc: 0.4810, Test Acc: 0.4122\n",
            "Epoch 79: Loss: 2.1081, Train Acc: 0.4806, Test Acc: 0.4173\n",
            "Epoch 80: Loss: 2.1002, Train Acc: 0.4831, Test Acc: 0.4148\n",
            "Epoch 81: Loss: 2.0860, Train Acc: 0.4889, Test Acc: 0.4188\n",
            "Epoch 82: Loss: 2.1032, Train Acc: 0.4851, Test Acc: 0.4082\n",
            "Epoch 83: Loss: 2.0867, Train Acc: 0.4871, Test Acc: 0.4184\n",
            "Epoch 84: Loss: 2.0876, Train Acc: 0.4888, Test Acc: 0.4093\n",
            "Epoch 85: Loss: 2.0889, Train Acc: 0.4870, Test Acc: 0.4122\n",
            "Epoch 86: Loss: 2.0823, Train Acc: 0.4931, Test Acc: 0.4111\n",
            "Epoch 87: Loss: 2.0853, Train Acc: 0.4922, Test Acc: 0.4162\n",
            "Epoch 88: Loss: 2.0817, Train Acc: 0.4922, Test Acc: 0.4170\n",
            "Epoch 89: Loss: 2.0638, Train Acc: 0.4959, Test Acc: 0.4235\n",
            "Epoch 90: Loss: 2.0695, Train Acc: 0.4875, Test Acc: 0.4090\n",
            "Epoch 91: Loss: 2.0657, Train Acc: 0.4939, Test Acc: 0.4195\n",
            "Epoch 92: Loss: 2.0576, Train Acc: 0.4922, Test Acc: 0.4253\n",
            "Epoch 93: Loss: 2.0470, Train Acc: 0.4963, Test Acc: 0.4206\n",
            "Epoch 94: Loss: 2.0449, Train Acc: 0.4944, Test Acc: 0.4144\n",
            "Epoch 95: Loss: 2.0583, Train Acc: 0.4957, Test Acc: 0.4188\n",
            "Epoch 96: Loss: 2.0489, Train Acc: 0.4983, Test Acc: 0.4213\n",
            "Epoch 97: Loss: 2.0457, Train Acc: 0.4919, Test Acc: 0.4144\n",
            "Epoch 98: Loss: 2.0375, Train Acc: 0.4922, Test Acc: 0.4173\n",
            "Epoch 99: Loss: 2.0383, Train Acc: 0.4988, Test Acc: 0.4192\n",
            "Epoch 100: Loss: 2.0417, Train Acc: 0.4998, Test Acc: 0.4162\n",
            "Epoch 101: Loss: 2.0371, Train Acc: 0.5014, Test Acc: 0.4192\n",
            "Epoch 102: Loss: 2.0038, Train Acc: 0.5109, Test Acc: 0.4279\n",
            "Epoch 103: Loss: 1.9903, Train Acc: 0.5083, Test Acc: 0.4246\n",
            "Epoch 104: Loss: 1.9826, Train Acc: 0.5118, Test Acc: 0.4308\n",
            "Epoch 105: Loss: 1.9811, Train Acc: 0.5146, Test Acc: 0.4283\n",
            "Epoch 106: Loss: 1.9838, Train Acc: 0.5117, Test Acc: 0.4272\n",
            "Epoch 107: Loss: 1.9853, Train Acc: 0.5125, Test Acc: 0.4232\n",
            "Epoch 108: Loss: 1.9879, Train Acc: 0.5134, Test Acc: 0.4235\n",
            "Epoch 109: Loss: 1.9822, Train Acc: 0.5130, Test Acc: 0.4279\n",
            "Epoch 110: Loss: 1.9826, Train Acc: 0.5143, Test Acc: 0.4290\n",
            "Epoch 111: Loss: 1.9673, Train Acc: 0.5159, Test Acc: 0.4290\n",
            "Epoch 112: Loss: 1.9914, Train Acc: 0.5178, Test Acc: 0.4297\n",
            "Epoch 113: Loss: 1.9782, Train Acc: 0.5153, Test Acc: 0.4243\n",
            "Epoch 114: Loss: 1.9695, Train Acc: 0.5154, Test Acc: 0.4308\n",
            "Epoch 115: Loss: 1.9810, Train Acc: 0.5184, Test Acc: 0.4301\n",
            "Epoch 116: Loss: 1.9773, Train Acc: 0.5184, Test Acc: 0.4268\n",
            "Epoch 117: Loss: 1.9678, Train Acc: 0.5183, Test Acc: 0.4301\n",
            "Epoch 118: Loss: 1.9746, Train Acc: 0.5174, Test Acc: 0.4283\n",
            "Epoch 119: Loss: 1.9694, Train Acc: 0.5149, Test Acc: 0.4246\n",
            "Epoch 120: Loss: 1.9675, Train Acc: 0.5178, Test Acc: 0.4294\n",
            "Epoch 121: Loss: 1.9762, Train Acc: 0.5179, Test Acc: 0.4312\n",
            "Epoch 122: Loss: 1.9678, Train Acc: 0.5173, Test Acc: 0.4272\n",
            "Epoch 123: Loss: 1.9651, Train Acc: 0.5147, Test Acc: 0.4272\n",
            "Epoch 124: Loss: 1.9736, Train Acc: 0.5180, Test Acc: 0.4319\n",
            "Epoch 125: Loss: 1.9584, Train Acc: 0.5155, Test Acc: 0.4308\n",
            "Epoch 126: Loss: 1.9554, Train Acc: 0.5192, Test Acc: 0.4312\n",
            "Epoch 127: Loss: 1.9489, Train Acc: 0.5165, Test Acc: 0.4308\n",
            "Epoch 128: Loss: 1.9622, Train Acc: 0.5180, Test Acc: 0.4294\n",
            "Epoch 129: Loss: 1.9530, Train Acc: 0.5208, Test Acc: 0.4319\n",
            "Epoch 130: Loss: 1.9621, Train Acc: 0.5178, Test Acc: 0.4301\n",
            "Epoch 131: Loss: 1.9598, Train Acc: 0.5232, Test Acc: 0.4334\n",
            "Epoch 132: Loss: 1.9553, Train Acc: 0.5212, Test Acc: 0.4326\n",
            "Epoch 133: Loss: 1.9483, Train Acc: 0.5197, Test Acc: 0.4323\n",
            "Epoch 134: Loss: 1.9471, Train Acc: 0.5176, Test Acc: 0.4308\n",
            "Epoch 135: Loss: 1.9517, Train Acc: 0.5234, Test Acc: 0.4297\n",
            "Epoch 136: Loss: 1.9528, Train Acc: 0.5243, Test Acc: 0.4323\n",
            "Epoch 137: Loss: 1.9576, Train Acc: 0.5242, Test Acc: 0.4294\n",
            "Epoch 138: Loss: 1.9576, Train Acc: 0.5218, Test Acc: 0.4330\n",
            "Epoch 139: Loss: 1.9583, Train Acc: 0.5212, Test Acc: 0.4315\n",
            "Epoch 140: Loss: 1.9518, Train Acc: 0.5236, Test Acc: 0.4370\n",
            "Epoch 141: Loss: 1.9456, Train Acc: 0.5230, Test Acc: 0.4381\n",
            "Epoch 142: Loss: 1.9524, Train Acc: 0.5238, Test Acc: 0.4323\n",
            "Epoch 143: Loss: 1.9576, Train Acc: 0.5226, Test Acc: 0.4301\n",
            "Epoch 144: Loss: 1.9501, Train Acc: 0.5255, Test Acc: 0.4334\n",
            "Epoch 145: Loss: 1.9576, Train Acc: 0.5244, Test Acc: 0.4359\n",
            "Epoch 146: Loss: 1.9470, Train Acc: 0.5224, Test Acc: 0.4345\n",
            "Epoch 147: Loss: 1.9446, Train Acc: 0.5243, Test Acc: 0.4359\n",
            "Epoch 148: Loss: 1.9430, Train Acc: 0.5252, Test Acc: 0.4366\n",
            "Epoch 149: Loss: 1.9418, Train Acc: 0.5235, Test Acc: 0.4294\n",
            "Epoch 150: Loss: 1.9395, Train Acc: 0.5277, Test Acc: 0.4312\n",
            "Epoch 151: Loss: 1.9330, Train Acc: 0.5246, Test Acc: 0.4341\n",
            "Epoch 152: Loss: 1.9334, Train Acc: 0.5270, Test Acc: 0.4377\n",
            "Epoch 153: Loss: 1.9376, Train Acc: 0.5282, Test Acc: 0.4366\n",
            "Epoch 154: Loss: 1.9281, Train Acc: 0.5275, Test Acc: 0.4352\n",
            "Epoch 155: Loss: 1.9398, Train Acc: 0.5274, Test Acc: 0.4345\n",
            "Epoch 156: Loss: 1.9334, Train Acc: 0.5284, Test Acc: 0.4374\n",
            "Epoch 157: Loss: 1.9343, Train Acc: 0.5274, Test Acc: 0.4355\n",
            "Epoch 158: Loss: 1.9227, Train Acc: 0.5264, Test Acc: 0.4366\n",
            "Epoch 159: Loss: 1.9295, Train Acc: 0.5282, Test Acc: 0.4330\n",
            "Epoch 160: Loss: 1.9235, Train Acc: 0.5265, Test Acc: 0.4388\n",
            "Epoch 161: Loss: 1.9335, Train Acc: 0.5285, Test Acc: 0.4355\n",
            "Epoch 162: Loss: 1.9245, Train Acc: 0.5295, Test Acc: 0.4363\n",
            "Epoch 163: Loss: 1.9331, Train Acc: 0.5268, Test Acc: 0.4359\n",
            "Epoch 164: Loss: 1.9362, Train Acc: 0.5285, Test Acc: 0.4388\n",
            "Epoch 165: Loss: 1.9290, Train Acc: 0.5275, Test Acc: 0.4355\n",
            "Epoch 166: Loss: 1.9262, Train Acc: 0.5294, Test Acc: 0.4355\n",
            "Epoch 167: Loss: 1.9274, Train Acc: 0.5294, Test Acc: 0.4352\n",
            "Epoch 168: Loss: 1.9239, Train Acc: 0.5285, Test Acc: 0.4341\n",
            "Epoch 169: Loss: 1.9176, Train Acc: 0.5284, Test Acc: 0.4370\n",
            "Epoch 170: Loss: 1.9282, Train Acc: 0.5302, Test Acc: 0.4359\n",
            "Epoch 171: Loss: 1.9268, Train Acc: 0.5295, Test Acc: 0.4363\n",
            "Epoch 172: Loss: 1.9245, Train Acc: 0.5297, Test Acc: 0.4352\n",
            "Epoch 173: Loss: 1.9115, Train Acc: 0.5293, Test Acc: 0.4345\n",
            "Epoch 174: Loss: 1.9212, Train Acc: 0.5285, Test Acc: 0.4345\n",
            "Epoch 175: Loss: 1.9238, Train Acc: 0.5296, Test Acc: 0.4319\n",
            "Epoch 176: Loss: 1.9245, Train Acc: 0.5299, Test Acc: 0.4370\n",
            "Epoch 177: Loss: 1.9167, Train Acc: 0.5291, Test Acc: 0.4377\n",
            "Epoch 178: Loss: 1.9210, Train Acc: 0.5297, Test Acc: 0.4355\n",
            "Epoch 179: Loss: 1.9163, Train Acc: 0.5299, Test Acc: 0.4330\n",
            "Epoch 180: Loss: 1.9294, Train Acc: 0.5289, Test Acc: 0.4352\n",
            "Epoch 181: Loss: 1.9192, Train Acc: 0.5306, Test Acc: 0.4366\n",
            "Epoch 182: Loss: 1.9244, Train Acc: 0.5305, Test Acc: 0.4352\n",
            "Epoch 183: Loss: 1.9280, Train Acc: 0.5297, Test Acc: 0.4374\n",
            "Epoch 184: Loss: 1.9327, Train Acc: 0.5275, Test Acc: 0.4359\n",
            "Epoch 185: Loss: 1.9272, Train Acc: 0.5293, Test Acc: 0.4370\n",
            "Epoch 186: Loss: 1.9241, Train Acc: 0.5291, Test Acc: 0.4377\n",
            "Epoch 187: Loss: 1.9040, Train Acc: 0.5290, Test Acc: 0.4381\n",
            "Epoch 188: Loss: 1.9234, Train Acc: 0.5288, Test Acc: 0.4377\n",
            "Epoch 189: Loss: 1.9273, Train Acc: 0.5286, Test Acc: 0.4345\n",
            "Epoch 190: Loss: 1.9138, Train Acc: 0.5312, Test Acc: 0.4345\n",
            "Epoch 191: Loss: 1.9147, Train Acc: 0.5293, Test Acc: 0.4348\n",
            "Epoch 192: Loss: 1.9200, Train Acc: 0.5293, Test Acc: 0.4374\n",
            "Epoch 193: Loss: 1.9355, Train Acc: 0.5306, Test Acc: 0.4388\n",
            "Epoch 194: Loss: 1.9098, Train Acc: 0.5312, Test Acc: 0.4374\n",
            "Epoch 195: Loss: 1.9225, Train Acc: 0.5322, Test Acc: 0.4377\n",
            "Epoch 196: Loss: 1.9161, Train Acc: 0.5308, Test Acc: 0.4374\n",
            "Epoch 197: Loss: 1.9301, Train Acc: 0.5316, Test Acc: 0.4377\n",
            "Epoch 198: Loss: 1.9161, Train Acc: 0.5313, Test Acc: 0.4341\n",
            "Epoch 199: Loss: 1.9181, Train Acc: 0.5312, Test Acc: 0.4377\n",
            "Epoch 200: Loss: 1.9284, Train Acc: 0.5324, Test Acc: 0.4374\n",
            "Epoch 201: Loss: 1.9185, Train Acc: 0.5308, Test Acc: 0.4359\n"
          ]
        }
      ],
      "source": [
        "best_params = study.best_params\n",
        "model = YogaPoseGNN(num_classes=len(le.classes_), hidden_dim=best_params['hidden_dim'], num_layers=best_params['num_layers'], dropout_rate=best_params['dropout_rate'])\n",
        "\n",
        "if best_params['optimizer'] == 'Adam':\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n",
        "else:\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'], momentum=0.9)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=best_params['step_size'], gamma=best_params['gamma'])\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=best_params['batch_size'], shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "for epoch in range(1, 201):\n",
        "    train_loss = train(model, optimizer, criterion, train_loader)\n",
        "    train_acc, train_preds, train_labels = test(model, train_loader)\n",
        "    test_acc, test_preds, test_labels = test(model, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f'Epoch {epoch+1}: Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c331fe3b-b8af-46f1-87b7-0dcc1ae003a4",
      "metadata": {
        "id": "c331fe3b-b8af-46f1-87b7-0dcc1ae003a4",
        "outputId": "0151416a-924e-4bb5-c395-f24ff99727ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.45      0.40        22\n",
            "           1       0.45      0.41      0.43        49\n",
            "           2       0.53      0.22      0.31        37\n",
            "           3       0.52      0.42      0.46        38\n",
            "           4       0.58      0.59      0.58        32\n",
            "           5       0.50      0.46      0.48        28\n",
            "           6       0.62      0.59      0.61        39\n",
            "           7       0.23      0.32      0.27        28\n",
            "           8       0.36      0.51      0.43        39\n",
            "           9       0.58      0.71      0.64        97\n",
            "          10       0.11      0.04      0.06        23\n",
            "          11       0.52      0.33      0.41        36\n",
            "          12       0.53      0.50      0.52        32\n",
            "          13       0.56      0.50      0.53        48\n",
            "          14       0.50      0.55      0.52        20\n",
            "          15       0.56      0.39      0.46        23\n",
            "          16       0.66      0.58      0.61        33\n",
            "          17       0.60      0.68      0.64        40\n",
            "          18       0.12      0.13      0.12        31\n",
            "          19       0.57      0.25      0.35        16\n",
            "          20       0.43      0.51      0.46        79\n",
            "          21       0.61      0.61      0.61        67\n",
            "          22       0.17      0.09      0.11        23\n",
            "          23       0.19      0.20      0.19        25\n",
            "          24       0.24      0.42      0.31        33\n",
            "          25       0.39      0.60      0.47        20\n",
            "          26       0.80      0.36      0.50        22\n",
            "          27       0.23      0.14      0.17        36\n",
            "          28       0.50      0.35      0.41        26\n",
            "          29       0.41      0.43      0.42        46\n",
            "          30       0.54      0.53      0.54        36\n",
            "          31       0.30      0.33      0.32        21\n",
            "          32       0.35      0.46      0.40        26\n",
            "          33       0.40      0.15      0.22        27\n",
            "          34       0.38      0.22      0.28        41\n",
            "          35       0.46      0.41      0.43        27\n",
            "          36       0.38      0.40      0.39        15\n",
            "          37       0.65      0.65      0.65        31\n",
            "          38       0.31      0.33      0.32        27\n",
            "          39       0.44      0.73      0.55        41\n",
            "          40       0.41      0.32      0.36        37\n",
            "          41       0.31      0.53      0.39        15\n",
            "          42       0.34      0.30      0.32        40\n",
            "          43       0.39      0.41      0.40        22\n",
            "          44       0.67      0.33      0.44        24\n",
            "          45       0.55      0.62      0.58        37\n",
            "          46       0.72      0.75      0.74        61\n",
            "          47       0.47      0.36      0.41        45\n",
            "          48       0.71      0.53      0.61        38\n",
            "          49       0.31      0.32      0.32        34\n",
            "          50       0.34      0.33      0.34        36\n",
            "          51       0.16      0.13      0.15        30\n",
            "          52       0.50      0.45      0.47        29\n",
            "          53       0.40      0.54      0.46        35\n",
            "          54       0.28      0.40      0.33        48\n",
            "          55       0.41      0.28      0.33        25\n",
            "          56       0.77      0.50      0.61        20\n",
            "          57       0.22      0.07      0.11        27\n",
            "          58       0.68      0.30      0.41        57\n",
            "          59       0.43      0.64      0.51        78\n",
            "          60       0.19      0.36      0.24        14\n",
            "          61       0.84      0.70      0.76        23\n",
            "          62       0.37      0.45      0.41        55\n",
            "          63       0.23      0.26      0.25        27\n",
            "          64       0.58      0.50      0.54        22\n",
            "          65       0.50      0.51      0.51        45\n",
            "          66       0.46      0.62      0.53        21\n",
            "          67       0.40      0.30      0.34        40\n",
            "          68       0.37      0.47      0.41        15\n",
            "          69       0.10      0.16      0.12        25\n",
            "          70       0.36      0.26      0.30        35\n",
            "          71       0.59      0.56      0.57        36\n",
            "          72       0.75      0.63      0.69        19\n",
            "          73       0.47      0.44      0.46        41\n",
            "          74       0.50      0.35      0.41        26\n",
            "          75       0.56      0.48      0.51        42\n",
            "          76       0.24      0.56      0.33        16\n",
            "          77       0.24      0.23      0.24        26\n",
            "          78       0.35      0.45      0.39        31\n",
            "          79       0.24      0.26      0.25        31\n",
            "          80       0.33      0.35      0.34        17\n",
            "          81       0.36      0.38      0.37        21\n",
            "\n",
            "    accuracy                           0.44      2746\n",
            "   macro avg       0.44      0.41      0.41      2746\n",
            "weighted avg       0.45      0.44      0.43      2746\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Classification Report:\")\n",
        "print(classification_report(test_labels, test_preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bf58e9-43a0-49d5-828f-b83b8cc5a48f",
      "metadata": {
        "id": "c1bf58e9-43a0-49d5-828f-b83b8cc5a48f",
        "outputId": "4df726a6-79ed-4aae-ac80-4a161d17464e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            "[[10  0  0 ...  0  0  0]\n",
            " [ 0 20  0 ...  0  1  0]\n",
            " [ 0  0  8 ...  0  0  0]\n",
            " ...\n",
            " [ 0  0  0 ...  8  0  1]\n",
            " [ 0  0  0 ...  0  6  0]\n",
            " [ 0  0  0 ...  0  1  8]]\n"
          ]
        }
      ],
      "source": [
        "conf_matrix = confusion_matrix(test_labels, test_preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}